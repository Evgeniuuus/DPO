# -*- coding: utf-8 -*-
"""Задание 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KwH1r7TZI11o1o19JZuTmcgeT9h1iK6u
"""

# ---------------------------------------------Задание №1 -----------------------------------------------
import pandas as pd

# Загрузим файл и посмотрим на данные
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Машинное обучение/HR.csv')
data.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Кодируем категориальные переменные
data_encoded = data.copy()
le_department = LabelEncoder()
le_salary = LabelEncoder()

data_encoded['department'] = le_department.fit_transform(data['department'])
data_encoded['salary'] = le_salary.fit_transform(data['salary'])

# Масштабируем числовые переменные для моделей
scaler = StandardScaler()
scaled_columns = ['satisfaction_level', 'last_evaluation', 'number_project',
                  'average_montly_hours', 'time_spend_company']
data_encoded[scaled_columns] = scaler.fit_transform(data_encoded[scaled_columns])

# Разделение данных для задачи 1
X_task1 = data_encoded.drop('left', axis=1)
y_task1 = data_encoded['left']
X_train_task1, X_test_task1, y_train_task1, y_test_task1 = train_test_split(X_task1, y_task1, test_size=0.2, random_state=42)

X_task1.head()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, accuracy_score

# Логистическая регрессия
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train_task1, y_train_task1)
y_pred_log = log_reg.predict(X_test_task1)
acc_log = accuracy_score(y_test_task1, y_pred_log)

# Случайный лес
rf = RandomForestClassifier(random_state=42, n_estimators=100)
rf.fit(X_train_task1, y_train_task1)
y_pred_rf = rf.predict(X_test_task1)
acc_rf = accuracy_score(y_test_task1, y_pred_rf)

# Градиентный бустинг
gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train_task1, y_train_task1)
y_pred_gb = gb.predict(X_test_task1)
acc_gb = accuracy_score(y_test_task1, y_pred_gb)

# Ансамблирование
voting_clf = VotingClassifier(estimators=[
    ('log_reg', log_reg),
    ('rf', rf),
    ('gb', gb)
], voting='hard')

voting_clf.fit(X_train_task1, y_train_task1)
y_pred_voting = voting_clf.predict(X_test_task1)
acc_voting = accuracy_score(y_test_task1, y_pred_voting)

# Вывод результатов
{
    "Logistic Regression": acc_log,
    "Random Forest": acc_rf,
    "Gradient Boosting": acc_gb,
    "Voting Classifier": acc_voting
}

"""Результаты точности моделей для предсказания текучести кадров:

- Логистическая регрессия: 75.77%
- Случайный лес: 98.83%
- Градиентный бустинг: 97.37%
- Ансамблирование: 97.83%
<p> Увы, но точность >99% нигде не наблюдается. Зато случайный лес дал наибольшую.

"""

# ---------------------------------------------Задание №2 -----------------------------------------------